<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark.">
  <meta name="keywords" content="Sa2VA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yuanhaobo.me">Haobo Yuan</a><sup>1</sup>,</span>
            </span>
            <span class="author-block">
              <a href="#">Yueyi Sun</a><sup>2</sup>,</span>
            </span>
            <span class="author-block">
              <a href="http://yanwei-li.com">Yanwei Li</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://zhang-tao-whu.github.io">Tao Zhang</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://xdeng7.github.io/xqdeng77.github.io/">Xueqing Deng</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://henghuiding.com">Henghui Ding</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="http://luqi.info">Lu Qi</a><sup>5</sup>
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/anranwang/home">Anran Wang</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="https://lxtgh.github.io">Xiangtai Li</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UC Merced</span>
            <span class="author-block"><sup>2</sup>PKU</span>
            <span class="author-block"><sup>3</sup>NTU</span>
            <span class="author-block"><sup>4</sup>CUHK</span>
            <span class="author-block"><sup>5</sup>WHU</span>
            <span class="author-block"><sup>6</sup>FDU</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2512.05091.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2512.05091"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/bytedance/Sa2VA/tree/main/projects/vrt_sa2va"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/HarborYuan/VisualReasoningTracer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/fig_teaser.jpg"
           alt="Visual Reasoning Tracer teaser"
           style="max-width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        Visual Reasoning Tracer grounds object-level reasoning across diverse visual scenes.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video0">
          <video poster="" id="video0" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/reasoning_process_0.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/reasoning_process_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/reasoning_process_2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/reasoning_process_3.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/reasoning_process_4.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-video5">
          <video poster="" id="video5" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/reasoning_process_5.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-video6">
          <video poster="" id="video6" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/reasoning_process_6.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-video7">
          <video poster="" id="video7" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/reasoning_process_7.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-video8">
          <video poster="" id="video8" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/reasoning_process_8.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-video9">
          <video poster="" id="video9" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/reasoning_process_9.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering. However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result. This contrasts with human intelligence, which naturally operates through a chain of visual reasoning. To address this limitation, we introduce the Visual Reasoning Tracer (VRT) task, which requires models to not only localize the target object but also explicitly predict the intermediate objects that form the reasoning path. To advance research in this area, we contribute: (1) VRT-Bench, a human-annotated benchmark for evaluating visual reasoning; (2) a new metric for assessing the quality of reasoning traces; and (3) VRT-80k, a large-scale dataset for reasoning model training. Our experiments reveal that while existing models often produce the correct final output, they struggle to ground their intermediate reasoning. In contrast, models trained on VRT-80k achieve substantial improvements in tracing the reasoning path. All benchmarks and code are available <a href="https://harboryuan.github.io/visual-reasoning-tracer">here</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section is-light">
  <div class="container is-max-desktop">
    <!-- VRT-Bench Details. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">VRT-Bench Examples</h2>
        <div class="content has-text-justified">
          <p>
            VRT-Bench is a human-annotated benchmark for evaluating visual reasoning. Unlike traditional tasks that only focus on the final result, VRT-Bench requires models to explicitly predict the intermediate objects that form the reasoning path, providing fine-grained evidence for the final prediction.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column">
            <img src="./static/images/fig_bench1.jpg" alt="VRT-Bench Example 1" style="width: 100%; margin-bottom: 15px;">
            <img src="./static/images/fig_bench2.jpg" alt="VRT-Bench Example 2" style="width: 100%; margin-bottom: 15px;">
            <img src="./static/images/fig_bench3.jpg" alt="VRT-Bench Example 3" style="width: 100%; margin-bottom: 15px;">
            <img src="./static/images/fig_bench4.jpg" alt="VRT-Bench Example 4" style="width: 100%;">
          </div>
        </div>
      </div>
    </div>
    <!--/ VRT-Bench Details. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Comparison</h2>
    <div class="content has-text-justified">
      <p>
        Our R-Sa2VA model produces a significantly more complete and interpretable reasoning trace compared to the Gemini 2.5 Pro and Qwen3-VL baselines. For the same query, R-Sa2VA sequentially identifies intermediate objects, clearly linking textual reasoning to the corresponding visual traces with dedicated segmentation masks. In contrast, baseline models often only segment the final answer or lack an explicit multi-object reasoning path, leaving intermediate cues ungrounded. These visualizations demonstrate that while existing pipelines can often locate the target, our model exposes a coherent, object-level visual reasoning process.
      </p>
    </div>
    <div class="columns is-centered has-text-centered is-multiline">
      <div class="column is-half">
        <h2 class="title is-5">GT</h2>
        <img src="./static/images/comp1_gt.jpg" alt="Ground Truth" style="width: 100%;">
      </div>
      <div class="column is-half">
        <h2 class="title is-5">R-Sa2VA (Ours)</h2>
        <img src="./static/images/comp1_rs2va.jpg" alt="R-Sa2VA (Ours)" style="width: 100%;">
      </div>
      <div class="column is-half">
        <h2 class="title is-5">Gemini 2.5 Pro</h2>
        <img src="./static/images/comp1_gemini.jpg" alt="Gemini 2.5 Pro" style="width: 100%;">
      </div>
      <div class="column is-half">
        <h2 class="title is-5">Qwen3VL</h2>
        <img src="./static/images/comp1_qwen3.jpg" alt="Qwen3VL" style="width: 100%;">
      </div>
    </div>
  </div>
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{yuan2025vrt,
  author    = {Haobo Yuan and Yueyi Sun and Yanwei Li and Tao Zhang and Xueqing Deng and Henghui Ding and Lu Qi and Anran Wang and Xiangtai Li and Ming-Hsuan Yang},
  title     = {Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark},
  journal   = {arXiv pre-print},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Great thanks to <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> for the source code of this website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
